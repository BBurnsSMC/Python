{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apJ5hVH5dMCn"
   },
   "source": [
    "# Import Packages, Modules, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "xjQEIcjSWOC1"
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import LogisticRegression #Linear regression proc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import RFE #Recursive Feature Elimination -Selects variables based on performance in model.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.stats import spearmanr # Function to calculate SpearmanR coefficient.\n",
    "from trueskill import TrueSkill, Rating, rate_1vs1 #Calculate advanced stat (similar to ELO)\n",
    "import statsmodels.api as sm  \n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from subprocess import check_output\n",
    "\n",
    "# Import packages to use for data exploration (graphs/plots)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "# Makes your plots appear in your notebook instead of creating a file somewhere\n",
    "%matplotlib inline\n",
    "\n",
    "#Import module to create our training and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "print(check_output([\"dir\", \"/b\", \"/a-d\",\"Data\\Stage2UpdatedDataFiles\"], shell=True).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zjC02D70cSYR"
   },
   "outputs": [],
   "source": [
    "# Create Data Frames\n",
    "# Read csv files into dataframes you can use in you notebook.\n",
    "data_dir = 'Data/Stage2UpdatedDataFiles/'\n",
    "\n",
    "# Regular Seasons Stats\n",
    "df_SeasDetResults = pd.read_csv(data_dir + 'RegularSeasonDetailedResults.csv')\n",
    "\n",
    "# Regular Seasons Stats\n",
    "df_TrnDetResults = pd.read_csv(data_dir + 'NCAATourneyDetailedResults.csv')\n",
    "\n",
    "# Use this to get team name from number\n",
    "df_teams = pd.read_csv(data_dir + 'Teams.csv')\n",
    "\n",
    "# Tournament Seeds\n",
    "df_TournSeeds = pd.read_csv(data_dir + 'NCAATourneySeeds.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnTMswS0fjU1"
   },
   "source": [
    "**Check to see if data is in frames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1518197913206,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "osLovJEnY35P",
    "outputId": "a861c324-76ad-4a85-c47e-3f32904e73a8"
   },
   "outputs": [],
   "source": [
    "df_SeasDetResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1518197915792,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "vnQJHJpcZmKV",
    "outputId": "5d6675be-b100-4b1d-f3e9-3a5b87553d86"
   },
   "outputs": [],
   "source": [
    "df_teams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1518197917368,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "v4-5k1TEbcEf",
    "outputId": "3d02a7dc-1d8c-426f-a07b-479e04ca44e4"
   },
   "outputs": [],
   "source": [
    "df_TrnDetResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_TournSeeds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9vCsz8xfsDs"
   },
   "source": [
    "# Prep the Regular Season Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "aHQdYcEXiroY"
   },
   "outputs": [],
   "source": [
    "# Merge in win seeds\n",
    "df_SeasDetResults = df_SeasDetResults.merge(df_TournSeeds, how='left', left_on=['Season', 'WTeamID'], \n",
    "                        right_on=['Season', 'TeamID'])\n",
    "\n",
    "# Rename column for win seed\n",
    "df_SeasDetResults = df_SeasDetResults.rename(columns={'Seed':'Wseed'})\n",
    "\n",
    "# Drop Team field \n",
    "df_SeasDetResults=df_SeasDetResults.drop(['TeamID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "v6rpBAqvjjjB"
   },
   "outputs": [],
   "source": [
    "# Merge in loss seeds\n",
    "df_SeasDetResults = df_SeasDetResults.merge(df_TournSeeds, how='left', left_on=['Season', 'LTeamID'], \n",
    "                        right_on=['Season', 'TeamID'])\n",
    "\n",
    "# Rename column for loss seed\n",
    "df_SeasDetResults = df_SeasDetResults.rename(columns={'Seed':'Lseed'})\n",
    "\n",
    "# Drop Team field \n",
    "df_SeasDetResults=df_SeasDetResults.drop(['TeamID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "GA0QVMiMjjsD"
   },
   "outputs": [],
   "source": [
    "# Prep Lseed and Wseed columns so that function to parse ints will work.\n",
    "df_SeasDetResults.fillna(value='Y00', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "iEOtEiqkjjvO"
   },
   "outputs": [],
   "source": [
    "# Parse int from varchar Wseed field \n",
    "def seed_to_int(seed):\n",
    "    \"\"\"Get just the digits from the seeding. Return as int\"\"\"\n",
    "    s_int = int(seed[1:3])\n",
    "    return s_int\n",
    "df_SeasDetResults['seed'] = df_SeasDetResults.Wseed.apply(seed_to_int)\n",
    "df_SeasDetResults.drop(labels=['Wseed'], inplace=True, axis=1) # This is the string label\n",
    "\n",
    "# Rename column for win seed\n",
    "df_SeasDetResults = df_SeasDetResults.rename(columns={'seed':'Wseed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ejt0dJO_jjyd"
   },
   "outputs": [],
   "source": [
    "# Parse int from varchar Lseed field\n",
    "def seed_to_int(seed):\n",
    "    \"\"\"Get just the digits from the seeding. Return as int\"\"\"\n",
    "    s_int = int(seed[1:3])\n",
    "    return s_int\n",
    "df_SeasDetResults['seed'] = df_SeasDetResults.Lseed.apply(seed_to_int)\n",
    "df_SeasDetResults.drop(labels=['Lseed'], inplace=True, axis=1) # This is the string label\n",
    "\n",
    "# Rename column for loss seed\n",
    "df_SeasDetResults = df_SeasDetResults.rename(columns={'seed':'Lseed'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't remove non-seeded games.\n",
    "\n",
    "The next cell is the main difference between this and the model actually submitted.  The goal here is just to see how we would have done if we had used a larger sample size of games rather than only games with seeded teams against seeded teams -which may have led to issues of small sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_92tAza4jj1E"
   },
   "outputs": [],
   "source": [
    "# Create dataframe for games only of seeded teams vs seeded teams.\n",
    "df_SeedsOnly = df_SeasDetResults.copy(deep=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 139,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 603,
     "status": "ok",
     "timestamp": 1518197930636,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "0i1AMkb1lfAY",
    "outputId": "17f14c84-410f-4f68-f95c-72d4d71ea6fa"
   },
   "outputs": [],
   "source": [
    "# Add variable to tell us if this is a tourney game or not.\n",
    "# Not a predictor, but just in case we want to know later on.\n",
    "df_SeedsOnly['IsTourneyGame'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9OyG0_1ClUY3"
   },
   "source": [
    "# Prep the Tournament Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1518197940603,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "176L-dAPlbbz",
    "outputId": "d3b0132b-513c-4a23-f887-810e61863519"
   },
   "outputs": [],
   "source": [
    "# Checking out the Tournament Detail Results Data Frame\n",
    "df_TrnDetResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1518197942090,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "T1XfJJnNjj9P",
    "outputId": "7b51340f-7538-4101-caf9-16e2f81cad62"
   },
   "outputs": [],
   "source": [
    "# Checking out the Tournament Seeds DataFrame\n",
    "df_TournSeeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1518197943492,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "sZUgcJqqjj_6",
    "outputId": "9b703c59-e207-4e62-cdb0-be13fea01bef"
   },
   "outputs": [],
   "source": [
    "# Prep Dataframes to be merged\n",
    "df_TournStats_ToMerge = df_TrnDetResults.copy(deep=True)\n",
    "df_TournStats_ToMerge.head()\n",
    "\n",
    "df_TournSeeds_ToMerge = df_TournSeeds.copy(deep=True)\n",
    "df_TournSeeds_ToMerge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1518197957308,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "dszFF2Stnb1M",
    "outputId": "85217bda-674a-463e-f285-347f6b8661ae"
   },
   "outputs": [],
   "source": [
    "df_TrnDetResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "vG66atEmnb4Q"
   },
   "outputs": [],
   "source": [
    "# Merge in win seeds\n",
    "df_TrnDetResults = df_TrnDetResults.merge(df_TournSeeds, how='left', left_on=['Season', 'WTeamID'], \n",
    "                        right_on=['Season', 'TeamID'])\n",
    "\n",
    "# Rename column for win seed\n",
    "df_TrnDetResults = df_TrnDetResults.rename(columns={'Seed':'Wseed'})\n",
    "\n",
    "# Drop Team field \n",
    "df_TrnDetResults=df_TrnDetResults.drop(['TeamID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "x0R_P37c64A6"
   },
   "outputs": [],
   "source": [
    "# Merge in loss seeds\n",
    "df_TrnDetResults = df_TrnDetResults.merge(df_TournSeeds, how='left', left_on=['Season', 'LTeamID'], \n",
    "                        right_on=['Season', 'TeamID'])\n",
    "\n",
    "# Rename column for loss seed\n",
    "df_TrnDetResults = df_TrnDetResults.rename(columns={'Seed':'Lseed'})\n",
    "\n",
    "# Drop Team field \n",
    "df_TrnDetResults=df_TrnDetResults.drop(['TeamID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3Rr6vV-u63-e"
   },
   "outputs": [],
   "source": [
    "  # Prep Lseed and Wseed columns so that function to parse ints will work.\n",
    "df_TrnDetResults.fillna(value='Y00', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UHeEtlUG637d"
   },
   "outputs": [],
   "source": [
    "# Parse int from varchar Wseed field \n",
    "def seed_to_int(seed):\n",
    "    \"\"\"Get just the digits from the seeding. Return as int\"\"\"\n",
    "    s_int = int(seed[1:3])\n",
    "    return s_int\n",
    "df_TrnDetResults['seed'] = df_TrnDetResults.Wseed.apply(seed_to_int)\n",
    "df_TrnDetResults.drop(labels=['Wseed'], inplace=True, axis=1) # This is the string label\n",
    "\n",
    "# Rename column for win seed\n",
    "df_TrnDetResults = df_TrnDetResults.rename(columns={'seed':'Wseed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8ikLmxU-634M"
   },
   "outputs": [],
   "source": [
    "# Parse int from varchar Lseed field\n",
    "def seed_to_int(seed):\n",
    "    \"\"\"Get just the digits from the seeding. Return as int\"\"\"\n",
    "    s_int = int(seed[1:3])\n",
    "    return s_int\n",
    "df_TrnDetResults['seed'] = df_TrnDetResults.Lseed.apply(seed_to_int)\n",
    "df_TrnDetResults.drop(labels=['Lseed'], inplace=True, axis=1) # This is the string label\n",
    "\n",
    "# Rename column for loss seed\n",
    "df_TrnDetResults = df_TrnDetResults.rename(columns={'seed':'Lseed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "81HkNTdI6318"
   },
   "outputs": [],
   "source": [
    "# Add variable to tell us if this is a tourney game or not.\n",
    "# Not a predictor, but just in case we want to know later on.\n",
    "df_TrnDetResults['IsTourneyGame'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hZkZk_Pk63yD"
   },
   "outputs": [],
   "source": [
    "df_TourneyOnly = df_TrnDetResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "uA-Wu-K7ncEt"
   },
   "outputs": [],
   "source": [
    "# Concatenate/append/union the regular season and tourney detail data sets together.\n",
    "\n",
    "df_CombStats = pd.concat([df_SeedsOnly, df_TourneyOnly])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 346,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1518197965456,
     "user": {
      "displayName": "Ben B",
      "photoUrl": "//lh3.googleusercontent.com/-G29XISVoSqE/AAAAAAAAAAI/AAAAAAAAAOM/gok8ZqPgdyo/s50-c-k-no/photo.jpg",
      "userId": "104324383755394097825"
     },
     "user_tz": 300
    },
    "id": "4X5vhcJo-yQ6",
    "outputId": "912b13db-7971-4276-ff18-add75da4d1ef"
   },
   "outputs": [],
   "source": [
    "df_CombStats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uoP81tkQoOxp"
   },
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zP8hBHKunc0m"
   },
   "outputs": [],
   "source": [
    "# Calculate Number of possessions (complex method)\n",
    " \n",
    "df_CombStats['W_poss'] = df_CombStats['WFGA']+ df_CombStats['WFGA3'] + 0.44 * df_CombStats['WFTA'] - 1.07 * \\\n",
    "                                            (df_CombStats['WOR'] / (df_CombStats['WOR'] + df_CombStats['WDR'])) * \\\n",
    "                                            (df_CombStats['WFGA'] + df_CombStats['WFGA3'] + df_CombStats['WFGM']+ df_CombStats['WFGM3']) + df_CombStats['WTO']\n",
    "df_CombStats['L_poss'] = df_CombStats['LFGA']+df_CombStats['LFGA3'] + 0.44 * df_CombStats['LFTA'] - 1.07 * \\\n",
    "                                            (df_CombStats['LOR'] / (df_CombStats['LOR'] + df_CombStats['LDR'])) * \\\n",
    "                                            (df_CombStats['LFGA'] + df_CombStats['LFGA3'] + df_CombStats['LFGM'] + df_CombStats['LFGM3']) + df_CombStats['LTO']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "BcpedlST3B9V"
   },
   "outputs": [],
   "source": [
    "# Calculate Number of Posessions (simple method)\n",
    "# Source:  https://www.burntorangenation.com/2011/10/19/2464697/advanced-basketball-statistics-understanding-possession-estimation\n",
    "\n",
    "df_CombStats['W_poss2'] = df_CombStats['WFGA']+ 0.475 * df_CombStats['WFTA'] - df_CombStats['WOR']  + df_CombStats['WTO']\n",
    "df_CombStats['L_poss2'] = df_CombStats['LFGA']+ 0.475 * df_CombStats['LFTA'] - df_CombStats['LOR']  + df_CombStats['LTO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bO-sK0cInc3f"
   },
   "outputs": [],
   "source": [
    "#Calculate offensive rating and defensive rating\n",
    "\n",
    "df_CombStats['W_OR'] = 100 * (df_CombStats['WScore']/df_CombStats['W_poss2'])\n",
    "df_CombStats['L_OR'] = 100 * (df_CombStats['LScore']/df_CombStats['L_poss2'])\n",
    "df_CombStats['W_DR'] = 100 * (df_CombStats['LScore']/df_CombStats['W_poss2'])\n",
    "df_CombStats['L_DR'] = 100 * (df_CombStats['WScore']/df_CombStats['L_poss2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "mz_fXQSbnc6Q"
   },
   "outputs": [],
   "source": [
    "# True shooting percentage and effective field goal percentage\n",
    "\n",
    "df_CombStats['W_eFG%']=  (df_CombStats['WFGA'] + df_CombStats['WFGA3'])/(df_CombStats['WFGA']+df_CombStats['WFGA3'])\n",
    "df_CombStats['L_eFG%']=  (df_CombStats['LFGA'] + df_CombStats['LFGA3'])/(df_CombStats['LFGA']+df_CombStats['LFGA3'])\n",
    "df_CombStats['W_TSP'] = df_CombStats['WScore'] / (2 * (df_CombStats['WFGA'] + df_CombStats['WFGA3'] + (0.44 * df_CombStats['WFGA'])))\n",
    "df_CombStats['L_TSP'] = df_CombStats['LScore'] / (2 * (df_CombStats['LFGA'] + df_CombStats['LFGA3'] + (0.44 * df_CombStats['LFGA'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MXGrfaTvndBY"
   },
   "outputs": [],
   "source": [
    "#Calculate turnovers per possessions\n",
    "\n",
    "df_CombStats['W_TO%'] = (df_CombStats['WTO']/df_CombStats['W_poss2'])\n",
    "df_CombStats['L_TO%'] = (df_CombStats['LTO']/df_CombStats['L_poss2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AaitLek4nc_O"
   },
   "outputs": [],
   "source": [
    "#Calculate free throw percentage\n",
    "\n",
    "df_CombStats['W_FT%'] = (df_CombStats['WFTM']/df_CombStats['WFTA'])\n",
    "df_CombStats['L_FT%'] = (df_CombStats['LFTM']/df_CombStats['LFTA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "P7a3H0iPonPp"
   },
   "outputs": [],
   "source": [
    "#Calculate OREB%\n",
    "\n",
    "df_CombStats['W_OREB%'] = (df_CombStats['WOR']/(df_CombStats['WOR']+df_CombStats['LDR']))\n",
    "df_CombStats['L_OREB%'] = (df_CombStats['LOR']/(df_CombStats['LDR']+df_CombStats['WDR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_lcuahKmonSv"
   },
   "outputs": [],
   "source": [
    "#Net Rating = Off.eff - Def.eff\n",
    "\n",
    "df_CombStats['W_NetRtg'] = df_CombStats.apply(lambda row:(row.W_OR - row.W_DR), axis=1)\n",
    "df_CombStats['L_NetRtg'] = df_CombStats.apply(lambda row:(row.L_OR - row.W_DR), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assist Ratio : Percentage of team possessions that end in assists\n",
    "\n",
    "df_CombStats['W_AstR'] = df_CombStats.apply(lambda row: 100 * row.WAst / (row.WFGA + 0.44*row.WFTA + row.WAst + row.WTO), axis=1)\n",
    "df_CombStats['L_AstR'] = df_CombStats.apply(lambda row: 100 * row.LAst / (row.LFGA + 0.44*row.LFTA + row.LAst + row.LTO), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FTA Rate : How good a team is at drawing fouls.\n",
    "\n",
    "df_CombStats['W_FTAR'] = df_CombStats.apply(lambda row: row.WFTA / row.WFGA, axis=1)\n",
    "df_CombStats['L_FTAR'] = df_CombStats.apply(lambda row: row.LFTA / row.LFGA, axis=1)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OREB% : Percentage of team offensive rebounds\n",
    "\n",
    "df_CombStats['W_ORP'] = df_CombStats.apply(lambda row: row.WOR / (row.WOR + row.LDR), axis=1)\n",
    "df_CombStats['L_ORP'] = df_CombStats.apply(lambda row: row.LOR / (row.LOR + row.WDR), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DREB% : Percentage of team defensive rebounds\n",
    "\n",
    "df_CombStats['W_DRP'] = df_CombStats.apply(lambda row: row.WDR / (row.WDR + row.LOR), axis=1)\n",
    "df_CombStats['L_DRP'] = df_CombStats.apply(lambda row: row.LDR / (row.LDR + row.WOR), axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "4aKscTofon_D"
   },
   "outputs": [],
   "source": [
    "#REB% : Percentage of team total rebounds\n",
    "\n",
    "df_CombStats['W_RP'] = df_CombStats.apply(lambda row: (row.WDR + row.WOR) / (row.WDR + row.WOR + row.LDR + row.LOR), axis=1)\n",
    "df_CombStats['L_RP'] = df_CombStats.apply(lambda row: (row.LDR + row.WOR) / (row.WDR + row.WOR + row.LDR + row.LOR), axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIE : Measure of a team's performance\n",
    "A high PIE % is highly correlated to winning. In fact, a team’s PIE rating and a team’s winning percentage correlate at an R square of .908 which indicates a \"strong\" correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Points Winning/Losing Team.  For calculating PIE\n",
    "df_CombStats['W_Pts'] = df_CombStats.apply(lambda row: 2*row.WFGM + row.WFGM3 + row.WFTM, axis=1)\n",
    "df_CombStats['L_Pts'] = df_CombStats.apply(lambda row: 2*row.LFGM + row.LFGM3 + row.LFTM, axis=1)\n",
    "\n",
    "\n",
    "#Calculate PIE\n",
    "df_CombStats['W_PIE'] = df_CombStats.apply(lambda row: (row.WDR + row.WOR) / (row.WDR + row.WOR + row.LDR + row.LOR), axis=1)\n",
    "wtmp = df_CombStats.apply(lambda row: row.W_Pts + row.WFGM + row.WFTM - row.WFGA - row.WFTA + row.WDR + 0.5*row.WOR + row.WAst +row.WStl + 0.5*row.WBlk - row.WPF - row.WTO, axis=1)\n",
    "ltmp = df_CombStats.apply(lambda row: row.L_Pts + row.LFGM + row.LFTM - row.LFGA - row.LFTA + row.LDR + 0.5*row.LOR + row.LAst +row.LStl + 0.5*row.LBlk - row.LPF - row.LTO, axis=1) \n",
    "df_CombStats['W_PIE'] = wtmp/(wtmp + ltmp)\n",
    "df_CombStats['L_PIE'] = ltmp/(wtmp + ltmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_CombStats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TrueSkill Rating\n",
    "\n",
    "#### TrueSkill is a rating system based on Bayesian inference, estimating each players skill as a gaussian-like Elo rating.\n",
    "#### See trueskill.org for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = TrueSkill(draw_probability=0.01) # 0.01 is arbitary small number\n",
    "beta = 25 / 6  # default value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Data Frames\n",
    "# Read csv files into dataframes you can use in you notebook.\n",
    "#data_dir = 'Data/DataFiles/'\n",
    "\n",
    "#df_tour =  pd.read_csv(data_dir + 'RegularSeasonCompactResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change IDs to concatenated Season and TeamID so we can get a ranking for each team in each season.\n",
    "df_CombStats['WTeamIDSeas'] = df_CombStats['Season'].astype(str) + df_CombStats['WTeamID'].astype(str)\n",
    "df_CombStats['LTeamIDSeas'] = df_CombStats['Season'].astype(str) + df_CombStats['LTeamID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tour = df_CombStats.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teamIds = np.unique(np.concatenate([df_tour.WTeamIDSeas.values, df_tour.LTeamIDSeas.values]))\n",
    "ratings = { tid:ts.Rating() for tid in teamIds }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create iterator to calculate TrueSkill for each season.\n",
    "\n",
    "def feed_season_results(season):\n",
    "    print(\"season = {}\".format(season))\n",
    "    df1 = df_tour[df_tour.Season == season]\n",
    "    for r in df1.itertuples():\n",
    "        ratings[r.WTeamIDSeas], ratings[r.LTeamIDSeas] = rate_1vs1(ratings[r.WTeamIDSeas], ratings[r.LTeamIDSeas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over each season to update TrueSkill.\n",
    "# Do this multiple times so that the ratings can refine on each update.\n",
    "for season in sorted(df_tour.Season.unique()) *30:\n",
    "    feed_season_results(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See ratings.  Just for giggles.  The TrueSkill rating is the mu; sigma is the confidence interval.\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull the ratings and the keys and ratings from the results of the iterator and combine in a df.\n",
    "keys = ratings.keys()\n",
    "rate = [r.mu for r in ratings.values()]\n",
    "\n",
    "df_trueskill = pd.DataFrame()\n",
    "df_trueskill['TeamSeasID']= keys\n",
    "df_trueskill['TSRating'] = rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the TeamID and Team Name so we can join it back to the data set.\n",
    "df_trueskill['Season'] = df_trueskill['TeamSeasID'].str[:4]\n",
    "df_trueskill['TeamID'] = df_trueskill['TeamSeasID'].str[4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_trueskill.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in win trueskill rating\n",
    "df_CombStats = df_CombStats.merge(df_trueskill, how='left', left_on=['WTeamIDSeas'], \n",
    "                        right_on=['TeamSeasID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_CombStats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop extra fields\n",
    "df_CombStats.drop(['TeamSeasID', 'Season_y', 'TeamID'],  inplace=True, axis=1)\n",
    "\n",
    "# Rename column for win trueskill\n",
    "df_CombStats = df_CombStats.rename(columns={'TSRating':'W_TSRating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in lose trueskill rating\n",
    "df_CombStats = df_CombStats.merge(df_trueskill, how='left', left_on=['LTeamIDSeas'], \n",
    "                        right_on=['TeamSeasID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop extra fields\n",
    "df_CombStats.drop(['TeamSeasID', 'Season', 'TeamID'],  inplace=True, axis=1)\n",
    "\n",
    "# Rename column for win trueskill\n",
    "df_CombStats = df_CombStats.rename(columns={'TSRating':'L_TSRating', 'Season_x':'Season'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_CombStats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Data Prep\n",
    "\n",
    "#### Drop unneeded variables\n",
    "#### Separate Test and Training Sets\n",
    "#### Calculate differentials for winners and losers and add Result column (this is the target)\n",
    "#### Remove all outliers more than 3 std from the mean\n",
    "#### Aggregate by season\n",
    "#### \n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy df to make starting over easier.\n",
    "\n",
    "df_CombStats2 = df_CombStats.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop extra fields\n",
    "df_CombStats2.drop(['DayNum', 'WLoc', 'NumOT', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', \n",
    "                   'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF',\n",
    "                   'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', \n",
    "                   'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF',\n",
    "                   'IsTourneyGame'\n",
    "                  ],  inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into wins and losses to be concatenated in order to get averages.  \n",
    "\n",
    "df_WinAgg = df_CombStats2.copy()\n",
    "\n",
    "df_WinAgg.drop(['LTeamID', 'LScore', 'Lseed', 'L_poss', 'L_poss2', 'L_OR', 'L_DR', 'L_eFG%', \n",
    "                   'L_TSP', 'L_TO%', 'L_FT%', 'L_OREB%', 'L_NetRtg', 'L_AstR', 'L_FTAR', 'L_ORP',\n",
    "                   'L_DRP', 'L_RP', 'L_Pts', 'L_PIE', 'L_TSRating','LTeamIDSeas' \n",
    "                  ],  inplace=True, axis=1)\n",
    "\n",
    "df_LoseAgg = df_CombStats2.copy()\n",
    "\n",
    "df_LoseAgg.drop(['WTeamID', 'WScore', 'Wseed', 'W_poss', 'W_poss2', 'W_OR', 'W_DR', 'W_eFG%', \n",
    "                   'W_TSP', 'W_TO%', 'W_FT%', 'W_OREB%', 'W_NetRtg', 'W_AstR', 'W_FTAR', 'W_ORP',\n",
    "                   'W_DRP', 'W_RP', 'W_Pts', 'W_PIE', 'W_TSRating','WTeamIDSeas' \n",
    "                  ],  inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns in win and lose aggs in order to concatenate.\n",
    "\n",
    "df_WinAgg = df_WinAgg.rename(columns={'WTeamID':'TeamID', 'WScore':'Score', 'Wseed':'Seed', 'W_poss':'Poss', 'W_poss2':'Poss2',\n",
    "                                      'W_OR':'OR', 'W_DR':'DR', 'W_eFG%':'eFGP','W_TSP':'TSP', 'W_TO%':'TOP', 'W_FT%':'FTP',\n",
    "                                      'W_OREB%':'ORebP', 'W_NetRtg':'NetRtg', 'W_AstR':'AstR', 'W_FTAR':'FTAR', 'W_ORP':'ORP',\n",
    "                                      'W_DRP':'DRP', 'W_RP':'RP', 'W_Pts':'Pts', 'W_PIE':'PIE', 'W_TSRating':'TSRtg',\n",
    "                                      'WTeamIDSeas':'TeamIDSeas'})\n",
    "\n",
    "df_LoseAgg = df_LoseAgg.rename(columns={'LTeamID':'TeamID', 'LScore':'Score', 'Lseed':'Seed', 'L_poss':'Poss', 'L_poss2':'Poss2',\n",
    "                                      'L_OR':'OR', 'L_DR':'DR', 'L_eFG%':'eFGP','L_TSP':'TSP', 'L_TO%':'TOP', 'L_FT%':'FTP',\n",
    "                                      'L_OREB%':'ORebP', 'L_NetRtg':'NetRtg', 'L_AstR':'AstR', 'L_FTAR':'FTAR', 'L_ORP':'ORP',\n",
    "                                      'L_DRP':'DRP', 'L_RP':'RP', 'L_Pts':'Pts', 'L_PIE':'PIE', 'L_TSRating':'TSRtg',\n",
    "                                      'LTeamIDSeas':'TeamIDSeas'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Append win/loss stats so we have all stats for every game, win or lose, for each team on separate rows.\n",
    "# Append is the same as Union in SQL\n",
    "# The product of this is what we can aggregate.\n",
    "\n",
    "df_AggStats = df_WinAgg.append(df_LoseAgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['Score'].mean() + (df_AggStats['Score'].std() *3)\n",
    "MinVal = df_AggStats['Score'].mean() - (df_AggStats['OR'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['Score'] > MaxVal, 'Score'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['Score'] < MinVal, 'Score'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['Poss'].mean() + (df_AggStats['Poss'].std() *3)\n",
    "MinVal = df_AggStats['Poss'].mean() - (df_AggStats['Poss'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['Poss'] > MaxVal, 'Poss'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['Poss'] < MinVal, 'Poss'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['Poss2'].mean() + (df_AggStats['Poss2'].std() *3)\n",
    "MinVal = df_AggStats['Poss2'].mean() - (df_AggStats['Poss2'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['Poss2'] > MaxVal, 'Poss2'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['Poss2'] < MinVal, 'Poss2'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['OR'].mean() + (df_AggStats['OR'].std() *3)\n",
    "MinVal = df_AggStats['OR'].mean() - (df_AggStats['OR'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['OR'] > MaxVal, 'OR'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['OR'] < MinVal, 'OR'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['eFGP'].mean() + (df_AggStats['eFGP'].std() *3)\n",
    "MinVal = df_AggStats['eFGP'].mean() - (df_AggStats['eFGP'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['eFGP'] > MaxVal, 'eFGP'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['eFGP'] < MinVal, 'eFGP'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['TSP'].mean() + (df_AggStats['TSP'].std() *3)\n",
    "MinVal = df_AggStats['TSP'].mean() - (df_AggStats['TSP'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['TSP'] > MaxVal, 'TSP'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['TSP'] < MinVal, 'TSP'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['TOP'].mean() + (df_AggStats['TOP'].std() *3)\n",
    "MinVal = df_AggStats['TOP'].mean() - (df_AggStats['TOP'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['TOP'] > MaxVal, 'TOP'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['TOP'] < MinVal, 'TOP'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['FTP'].mean() + (df_AggStats['FTP'].std() *3)\n",
    "MinVal = df_AggStats['FTP'].mean() - (df_AggStats['FTP'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['FTP'] > MaxVal, 'FTP'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['FTP'] < MinVal, 'FTP'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['ORebP'].mean() + (df_AggStats['ORebP'].std() *3)\n",
    "MinVal = df_AggStats['ORebP'].mean() - (df_AggStats['ORebP'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['ORebP'] > MaxVal, 'ORebP'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['ORebP'] < MinVal, 'ORebP'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['NetRtg'].mean() + (df_AggStats['NetRtg'].std() *3)\n",
    "MinVal = df_AggStats['NetRtg'].mean() - (df_AggStats['NetRtg'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['NetRtg'] > MaxVal, 'NetRtg'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['NetRtg'] < MinVal, 'NetRtg'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['AstR'].mean() + (df_AggStats['AstR'].std() *3)\n",
    "MinVal = df_AggStats['AstR'].mean() - (df_AggStats['AstR'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['AstR'] > MaxVal, 'AstR'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['AstR'] < MinVal, 'AstR'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['FTAR'].mean() + (df_AggStats['FTAR'].std() *3)\n",
    "MinVal = df_AggStats['FTAR'].mean() - (df_AggStats['FTAR'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['FTAR'] > MaxVal, 'FTAR'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['FTAR'] < MinVal, 'FTAR'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['ORP'].mean() + (df_AggStats['ORP'].std() *3)\n",
    "MinVal = df_AggStats['ORP'].mean() - (df_AggStats['ORP'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['ORP'] > MaxVal, 'ORP'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['ORP'] < MinVal, 'ORP'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['DRP'].mean() + (df_AggStats['DRP'].std() *3)\n",
    "MinVal = df_AggStats['DRP'].mean() - (df_AggStats['DRP'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['DRP'] > MaxVal, 'DRP'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['DRP'] < MinVal, 'DRP'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['RP'].mean() + (df_AggStats['RP'].std() *3)\n",
    "MinVal = df_AggStats['RP'].mean() - (df_AggStats['RP'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['RP'] > MaxVal, 'RP'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['RP'] < MinVal, 'RP'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['Pts'].mean() + (df_AggStats['Pts'].std() *3)\n",
    "MinVal = df_AggStats['Pts'].mean() - (df_AggStats['Pts'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['Pts'] > MaxVal, 'Pts'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['Pts'] < MinVal, 'Pts'] = MinVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the top and bottom threshold for outliers.\n",
    "MaxVal = df_AggStats['PIE'].mean() + (df_AggStats['PIE'].std() *3)\n",
    "MinVal = df_AggStats['PIE'].mean() - (df_AggStats['PIE'].std() *3)\n",
    "\n",
    "# Update outliers to equal the threshold values.\n",
    "df_AggStats.loc[df_AggStats['PIE'] > MaxVal, 'PIE'] = MaxVal\n",
    "df_AggStats.loc[df_AggStats['PIE'] < MinVal, 'PIE'] = MinVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average, Aggregate and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new df with averaged all team season stats\n",
    "# The reset_index is key so that I can join later using that column.\n",
    "\n",
    "df_AggStats2 = df_AggStats.groupby('TeamIDSeas', axis=0).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get win and loss team ID for each game in order to recombine with aggregated season stats. \n",
    "\n",
    "df_WinLoss = df_CombStats2[['Season', 'WTeamID', 'LTeamID', 'WTeamIDSeas', 'LTeamIDSeas']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge the average team stats for the win team.\n",
    "df_WinLoss = pd.merge(df_WinLoss, df_AggStats2, left_on='WTeamIDSeas', right_on='TeamIDSeas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up columns, and rename stats so they can be identified as the win team stats.\n",
    "df_WinLoss.drop(['Season_y', 'TeamID'],  inplace=True, axis=1)\n",
    "\n",
    "df_WinLoss = df_WinLoss.rename(columns={'Score':'WScore', 'Seed':'WSeed', 'Poss':'WPoss', 'Poss2':'WPoss2',\n",
    "                                      'OR':'WOR', 'DR':'WDR', 'eFGP':'WeFGP','TSP':'WTSP', 'TOP':'WTOP', 'FTP':'WFTP',\n",
    "                                      'ORebP':'WORebP', 'NetRtg':'WNetRtg', 'AstR':'WAstR', 'FTAR':'WFTAR', 'ORP':'WORP',\n",
    "                                      'DRP':'WDRP', 'RP':'WRP', 'Pts':'WPts', 'PIE':'WPIE', 'TSRtg':'WTSRtg',\n",
    "                                      'TeamIDSeas':'WTeamIDSeas'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat steps above for losing team with a copy of df_WinLoss so it doesn't overwrite our changes.\n",
    "\n",
    "df_WinLoss2 = df_WinLoss.copy()\n",
    "\n",
    "df_WinLoss2 = pd.merge(df_WinLoss2, df_AggStats2, left_on='LTeamIDSeas', right_on='TeamIDSeas')\n",
    "\n",
    "df_WinLoss2.drop(['Season_y', 'TeamID'],  inplace=True, axis=1)\n",
    "\n",
    "df_WinLoss2 = df_WinLoss2.rename(columns={'Score':'LScore', 'Seed':'LSeed', 'Poss':'LPoss', 'Poss2':'LPoss2',\n",
    "                                      'OR':'LOR', 'DR':'LDR', 'eFGP':'LeFGP','TSP':'LTSP', 'TOP':'LTOP', 'FTP':'LFTP',\n",
    "                                      'ORebP':'LORebP', 'NetRtg':'LNetRtg', 'AstR':'LAstR', 'FTAR':'LFTAR', 'ORP':'LORP',\n",
    "                                      'DRP':'LDRP', 'RP':'LRP', 'Pts':'LPts', 'PIE':'LPIE', 'TSRtg':'LTSRtg',\n",
    "                                      'TeamIDSeas':'LTeamIDSeas'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Differentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Winner Stats\n",
    "# if you set a dataframe = to another dataframe it will modify the the original data frame.  \n",
    "# Use copy to preserve and not accidentally do bad things and upset yourself later\n",
    "df_WinnerStats = df_WinLoss2.copy(deep=True)\n",
    "\n",
    "# Calculate Differentials\n",
    "#Also add a column called Result and set it to =1 for all winning rows.  This is our target variable for predication.  \n",
    "df_WinnerStats['diff_Score'] = df_WinLoss2.WScore  - df_WinLoss2.LScore \n",
    "df_WinnerStats['diff_Seed'] = df_WinLoss2.WSeed   - df_WinLoss2.LSeed  \n",
    "df_WinnerStats['diff_Poss'] = df_WinLoss2.WPoss   - df_WinLoss2.LPoss  \n",
    "df_WinnerStats['diff_Poss2'] = df_WinLoss2.WPoss2  - df_WinLoss2.LPoss2 \n",
    "df_WinnerStats['diff_OR'] = df_WinLoss2.WOR     - df_WinLoss2.LOR    \n",
    "df_WinnerStats['diff_DR'] = df_WinLoss2.WDR     - df_WinLoss2.LDR    \n",
    "df_WinnerStats['diff_eFGP'] = df_WinLoss2.WeFGP   - df_WinLoss2.LeFGP  \n",
    "df_WinnerStats['diff_TSP'] = df_WinLoss2.WTSP    - df_WinLoss2.LTSP   \n",
    "df_WinnerStats['diff_TOP'] = df_WinLoss2.WTOP    - df_WinLoss2.LTOP   \n",
    "df_WinnerStats['diff_FTP'] = df_WinLoss2.WFTP    - df_WinLoss2.LFTP   \n",
    "df_WinnerStats['diff_ORebP'] = df_WinLoss2.WORebP  - df_WinLoss2.LORebP \n",
    "df_WinnerStats['diff_NetRtg'] = df_WinLoss2.WNetRtg - df_WinLoss2.LNetRtg\n",
    "df_WinnerStats['diff_AstR'] = df_WinLoss2.WAstR   - df_WinLoss2.LAstR  \n",
    "df_WinnerStats['diff_FTAR'] = df_WinLoss2.WFTAR   - df_WinLoss2.LFTAR  \n",
    "df_WinnerStats['diff_ORP'] = df_WinLoss2.WORP    - df_WinLoss2.LORP   \n",
    "df_WinnerStats['diff_DRP'] = df_WinLoss2.WDRP    - df_WinLoss2.LDRP   \n",
    "df_WinnerStats['diff_RP'] = df_WinLoss2.WRP     - df_WinLoss2.LRP    \n",
    "df_WinnerStats['diff_Pts'] = df_WinLoss2.WPts    - df_WinLoss2.LPts   \n",
    "df_WinnerStats['diff_PIE'] = df_WinLoss2.WPIE    - df_WinLoss2.LPIE   \n",
    "df_WinnerStats['diff_TSRtg'] = df_WinLoss2.WTSRtg  - df_WinLoss2.LTSRtg \n",
    "df_WinnerStats['Result'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as above for Loser Stats\n",
    "\n",
    "df_LoserStats = df_WinLoss2.copy(deep=True)\n",
    "\n",
    "df_LoserStats['diff_Score'] = df_WinLoss2.LScore  - df_WinLoss2.WScore \n",
    "df_LoserStats['diff_Seed'] = df_WinLoss2.LSeed   - df_WinLoss2.WSeed  \n",
    "df_LoserStats['diff_Poss'] = df_WinLoss2.LPoss   - df_WinLoss2.WPoss  \n",
    "df_LoserStats['diff_Poss2'] = df_WinLoss2.LPoss2  - df_WinLoss2.WPoss2 \n",
    "df_LoserStats['diff_OR'] = df_WinLoss2.LOR     - df_WinLoss2.WOR    \n",
    "df_LoserStats['diff_DR'] = df_WinLoss2.LDR     - df_WinLoss2.WDR    \n",
    "df_LoserStats['diff_eFGP'] = df_WinLoss2.LeFGP   - df_WinLoss2.WeFGP  \n",
    "df_LoserStats['diff_TSP'] = df_WinLoss2.LTSP    - df_WinLoss2.WTSP   \n",
    "df_LoserStats['diff_TOP'] = df_WinLoss2.LTOP    - df_WinLoss2.WTOP   \n",
    "df_LoserStats['diff_FTP'] = df_WinLoss2.LFTP    - df_WinLoss2.WFTP   \n",
    "df_LoserStats['diff_ORebP'] = df_WinLoss2.LORebP  - df_WinLoss2.WORebP \n",
    "df_LoserStats['diff_NetRtg'] = df_WinLoss2.LNetRtg - df_WinLoss2.WNetRtg\n",
    "df_LoserStats['diff_AstR'] = df_WinLoss2.LAstR   - df_WinLoss2.WAstR  \n",
    "df_LoserStats['diff_FTAR'] = df_WinLoss2.LFTAR   - df_WinLoss2.WFTAR  \n",
    "df_LoserStats['diff_ORP'] = df_WinLoss2.LORP    - df_WinLoss2.WORP   \n",
    "df_LoserStats['diff_DRP'] = df_WinLoss2.LDRP    - df_WinLoss2.WDRP   \n",
    "df_LoserStats['diff_RP'] = df_WinLoss2.LRP     - df_WinLoss2.WRP    \n",
    "df_LoserStats['diff_Pts'] = df_WinLoss2.LPts    - df_WinLoss2.WPts   \n",
    "df_LoserStats['diff_PIE'] = df_WinLoss2.LPIE    - df_WinLoss2.WPIE   \n",
    "df_LoserStats['diff_TSRtg'] = df_WinLoss2.LTSRtg  - df_WinLoss2.WTSRtg \n",
    "df_LoserStats['Result'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Append win/loss stats so we have all our differentials, other stats, and target variable for all wins and losses on separate \n",
    "#rows in the same data set.\n",
    "# Append is the same as Union in SQL\n",
    "\n",
    "concats = [df_WinnerStats, df_LoserStats]\n",
    "df_Concats = df_WinnerStats.append(df_LoserStats)\n",
    "df_Concats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop unneeded/unwanted columns.\n",
    "df_Concats.drop(labels=[ 'WScore', 'LScore', 'WSeed', 'LSeed', 'WPoss', 'LPoss', 'WPoss2', 'LPoss2',\n",
    " 'WOR', 'LOR', 'WDR', 'LDR', 'WeFGP', 'LeFGP', 'WTSP', 'LTSP', 'WTOP', 'LTOP',\n",
    " 'WFTP', 'LFTP', 'WORebP', 'LORebP', 'WNetRtg', 'LNetRtg', 'WAstR', 'LAstR',\n",
    " 'WFTAR', 'LFTAR', 'WORP', 'LORP', 'WDRP', 'LDRP', 'WRP', 'LRP', 'WPts', 'LPts',\n",
    " 'WPIE', 'LPIE', 'WTSRtg', 'LTSRtg'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_Concats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Test and Training Sets (Should be saved to CSV to make permanent, but hey...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create training and test sets for the data.\n",
    "# df_train is the test set.  Use this one for analysis.\n",
    "# df_test is the test set.  Don't do anything to this one for now.\n",
    "\n",
    "df_train, df_test = train_test_split(df_Concats, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "\n",
    "# Set size of plot so that it's readable\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Produce heatmap\n",
    "sns.heatmap(df_train.corr(), square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create an array with all possible variables in it except the target. \n",
    "data_variables=df_train.columns.values.tolist()\n",
    "y=['Result']\n",
    "X=[i for i in data_variables if i not in y]\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add selected columns to a new array.\n",
    "cols=['diff_TSP', 'diff_NetRtg', 'diff_TSRtg']\n",
    "X=df_train[cols]\n",
    "y=df_train['Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run model with Statsmodels API\n",
    "import statsmodels.api as sm  \n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use SKLearn to prep data for some diagnostics and fit testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy is the proportion of true positives and true negatives vs false positives and false negatives.\n",
    "# In your confusin matrix, it's top-left + bottom-right / total.\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10-Fold Cross-Validation\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "modelCV = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Precision, Recall, F-Measure and Support\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "# The ROC curve measures the tradeoff between selecting as many True Positives as possible while avoiding False Positives.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a graph for the optimal number of features to use.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "# The \"accuracy\" scoring is proportional to the number of correct\n",
    "# classifications\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Actual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get submission and teams csv.  Initialize in df.\n",
    "\n",
    "df_predict = pd.read_csv(data_dir + 'SampleSubmissionStage2.csv')\n",
    "df_teams = pd.read_csv(data_dir + 'Teams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the IDs from the submission file.\n",
    "\n",
    "def get_year_t1_t2(id):\n",
    "    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n",
    "    return (int(x) for x in id.split('_'))\n",
    "\n",
    "for row in df_predict.itertuples():\n",
    "    year, t1, t2 = get_year_t1_t2(row.ID)\n",
    "    idx = row.Index\n",
    "    df_predict.at[idx, 'Year'] = year\n",
    "    df_predict.at[idx, 'Wteam'] = t1\n",
    "    df_predict.at[idx, 'Lteam'] = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab the data from before differentials were calculated to make the model, and use it to prep the new data.\\\n",
    "# Will need to get all the aggregated stats connected for the matchups in the tournament in order to predict the new games.\n",
    "# This is basically prepping for what is called \"out of sample\" prediction.  \n",
    "  # Meaning, now we're predicting stuff that wasn't in the original data sample we built the model on.  These are actual predictions.\n",
    "\n",
    "predcats = [df_WinnerStats, df_LoserStats]\n",
    "predcats = df_WinnerStats.append(df_LoserStats)\n",
    "predcats.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take only columns we need for calculating the differentials our model uses as features.\n",
    "predcats2 = predcats[['Season', 'WTeamID', 'LTeamID', 'WTSP','WNetRtg','WTSRtg', \n",
    "                       'LTSP','LNetRtg','LTSRtg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter to only take 2018 data because only 2018 teams are int he tourney.\n",
    "predcats3 = predcats2.loc[predcats2['Season'] == 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge win team aggregated 2018 data\n",
    "df_predict = df_predict.merge(predcats3, how='left', left_on=['Year', 'Wteam'], \n",
    "                        right_on=['Season', 'WTeamID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge lose team aggregated 2018 data\n",
    "df_predict = df_predict.merge(predcats3, how='left', left_on=['Year', 'Lteam'], \n",
    "                        right_on=['Season', 'LTeamID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unneeded columns.\n",
    "df_predict = df_predict[['ID', 'Year', 'Wteam', 'Lteam', 'WTSP_x', 'WNetRtg_x', 'WTSRtg_x', \n",
    "                         'LTSP_y', 'LNetRtg_y', 'LTSRtg_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate differentials for each of the matchups.\n",
    "df_predict['diff_TSP'] = df_predict['WTSP_x'] - df_predict['LTSP_y']\n",
    "df_predict['diff_NetRtg'] = df_predict['WNetRtg_x'] - df_predict['LNetRtg_y']\n",
    "df_predict['diff_TSRtg'] = df_predict['WTSRtg_x'] - df_predict['LTSRtg_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above appeared to give a cartesian product, so drop the extra duplicate columns that showed up.\n",
    "df_predict2 = df_predict.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the actual model intercept and coefficients\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(logreg.intercept_, logreg.coef_)\n",
    "print(logrreg.score(y_test, y_pred))\n",
    "\n",
    "# Ignore the error.  Not sure why it shows up, but it still gives us the intercept and coefficients.\n",
    "# Intercept is the first one.  Other coefficients I believe are in the order they were put into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use intercept and coefficients to get yhat (the prediction variable).\n",
    "# Note:  These are the log odds, which are very difficult to interpret.  We handle that next.\n",
    "df_predict2['yhat'] = (-0.0026083 + (df_predict2['diff_TSP'] * 0.16296741 ) - \n",
    "                             (df_predict2['diff_NetRtg'] * 0.01526614  ) + \n",
    "                             (df_predict2['diff_TSRtg'] * 0.28516537))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exponentiating yhat gives you the log odds.  This is transformed into a probabilty next.\n",
    "df_predict2['expyhat'] = np.exp(df_predict2['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get your probability.  This is you're probability that \"left team beats right team\" -based on the way the submission file is set up.\n",
    "# In general, it's your predictid probability that your result = 1.\n",
    "# You can take this one step further and set it to a binary where anything greater than 0.5 = 1; less than 0.5 = 0.\n",
    "df_predict2['prob'] = df_predict2['expyhat'] / (1 + df_predict2['expyhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just for giggles:  Exponentiate coefficients to convert to odds.\n",
    "# Not used for new predictions, but useful when trying to interpret the effects of the coefficients.\n",
    "# Interpretation:  For each unit increase in diff_TSP, the odds of result = 1 increase 1.352131 times.\n",
    "    # Which is to say, they more than double.\n",
    "np.exp(result.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_predict2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take only needed columns.\n",
    "df_final = df_predict2[['Year', 'Wteam', 'Lteam', 'prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in the team names.  Easier to fill out bracket that way.\n",
    "\n",
    "# Merge in win teams.\n",
    "df_final = df_final.merge(df_teams, how='left', left_on=['Wteam'], \n",
    "                        right_on=['TeamID'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename and drop columns for clarity.\n",
    "df_final = df_final.rename(columns={'TeamName':'WTeamName'})\n",
    "\n",
    "df_final=df_final.drop(['TeamID', 'FirstD1Season', 'LastD1Season'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge in lose teams.\n",
    "df_final = df_final.merge(df_teams, how='left', left_on=['Lteam'], \n",
    "                        right_on=['TeamID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename and drop columns for clarity.\n",
    "df_final = df_final.rename(columns={'TeamName':'LTeamName'})\n",
    "\n",
    "df_final=df_final.drop(['TeamID', 'FirstD1Season', 'LastD1Season'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save predictions to csv\n",
    "df_final.to_csv('MMPredicts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go fill out your bracket!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DID NOT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #10-Fold Cross-Validation\n",
    "# from sklearn import model_selection\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "# modelCV = LogisticRegression()\n",
    "# scoring = 'accuracy'\n",
    "# results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "# print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "# print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Calculate Precision, Recall, F-Measure and Support\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # ROC Curve\n",
    "# # The ROC curve measures the tradeoff between selecting as many True Positives as possible while avoiding False Positives.\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import roc_curve\n",
    "# logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('Log_ROC')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exponentiate coefficients to convert to odds.\n",
    "# Not used for new predictions, but useful when trying to interpret the effects of the coefficients.\n",
    "np.exp(result.params)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "March Madness 2018.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
